# Apache Spark 1.6 - Transform, Stage and Store
## Introduction
### Agenda
- Objective
- Problem Statement
- Introduction to Spark
- Initializing the job
- Create RDD using data from HDFS
- Read data from different file formats
- Standard Transformations
- Saving RDD back to HDFS
- Save data in different formats
- Solution

#### Objective
Convert a set of data values in a given format stored in HDFS into new data values or a new data format and write then into HDFS 
- Load RDD data from HDFS for use in Spark Application
- Write the result from an RDD back into HDFS using spark
- Read and write files in a variety of file formats
- Perform standard extract, transform, load (ETL) processes on data

#### Problem Statement
- Use retail_db data set
- Problem Statement
    - Get daily revenue by product considering completed and closed orders
    - Data need to be sorted by ascending order by date and then descending order by revenue computed for each product for each day
- Data for orders and order_items is available in HDFS
/public/retail_db/orders and /public/retail_db/order_items
- Data for products is available locally under /data/retail_db/products
- Final output need to be stored under
    - HDFS location - avro format
    /user/YOUR_USER_ID/daily_revenue_avro_python
    - HDFS location -text format 
    /user/YOUR_USER_ID/daily_revenue_txt_python
    - Local location /home/YOUR_USER_ID/daily_revenue_python
    - Solution need to be stored under
    /home/YOUR_USER_ID/daily_revenue_python.txt

### Introduction to spark
- Spark is Distributed computing framework
- Bunch of APIs to process data
- Higher level modules such as Data Frames/SQL, Streaming, MLLib and more
- Well integrated with python, scala, java, etc
- spark uses HDFS API to deal with file system
- It can run against any distributed or cloud file system- HDFS, S3, AzureBlob, etc
- Only Core spark and spark SQL (including Data Frames) is part of the curriculum for CCA spark and Hadoop developer. 
  CCA also requires some knowledge of Spark Streaming.
- Pre-requisites Programming language (Python or Scala)

------------------------------------------------------------------------------------------
Initializing the job
- Initializing using pyspark
    - Running in yarn mode (client or cluster mode)
    - Control arguments
    - Deciding on number of executors
    - Setting up additional properties

- Programmatic initialization of job
    - Create configuration object 
    - Create Spark Context object

--------------------------------------
pyspark --master yarn --conf spark.ui.port=12568
sc
--------------------------------------------------------------------------------------------
# Create RDD using Data from HDFS 
- RDD is extension to python list
- RDD - Resilient Distributed Dataset
    - In-memory
    - Distributed
    - Resilient

- Reading files form HDFS
- Quick overview of Transformations and Actions
- DAG and lazy evaluation
- Previewing the data using Actions
---------------------------------------
hadoop fs -ls /public/retail_db
help(sc)

orderItems = sc.textFile("/public/retail_db/order_items")
type(orderItems)
help(orderItems)

orderItems.first() # first row
for i in orderItems.take(10): print(i) # first 10 rows
------------------------------------------

#Lazy evaluation and Directed Acyclic Graph (DAG)
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems.map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))
revenuePerOrder = orderItemsMap.reduceByKey(lambda curr, next: curr + next)

orderItems.toDebugString()
orderItemsMap.toDebugString()
revenuePerOrder.toDebugSting()

for i  in revenuePerOrder.take(10): print(i)

revenuePerOrder.count()

orderItemsList = orderItems.collect()
type(orderItemList)
orderItem.count()

-----------------------------------------------------------------------------------------------------
# Create RDD using data from collection
- Create collection
- Use sc.parallelize to convert into RDD
- Real world use case
    - Read data from local file system
    - Convert into collection
    - Create RDD using sc.parallelize
----------------------------------------

l = range(1, 10000)
type(l)
lRRD = sc.parallelize(l)
type(lRDD)

hadoop fs -ls /public/retail_db/order_items # HDFS file directory
ls -ltr /data/retail_db # local file directory

productsRaw = open("/data/retail_db/products/part=00000).read().splitlines()
type(productsRaw)
productsRDD = sc.parallelize(productsRaw)

productRDD.first()
productRDD.take()
productRDD.count()
-------------------------------------------------------------------------------------------------
# Read Data from Different file formats
- Data Frame (Distributed collection with structure)
- APIs provided by sqlContext
- Support file format
    - orc
    - json
    - parquet
    - avro
- Previewing the data
    - show
------------------------------------------------
sc #Spark context
sqlContext #Hive Context

help(sqlContext) #load and #read
help(sqlContext.load)
help(sqlContext.read.json)
help(sqlContext.read.orc)
help(sqlContext.read.parquet)
help(sqlContext.read.text)
-----------------------------
# load and read json documents (both serve the same purpose)
hadoop fs -ls /public/ # second terminal
hadoop fs -ls /public/retail_db_json # second terminal

sqlContext.load("/public/retail_db_json/order_items", "json").show()
sqlContext.read.json("/public/retail_db_json/order_items").show()
---------------------------------------------------------------------------------------------------------------------
# Standard Transformations
- String Manipulation (Python)

orders = sc.textFile("/public/retail_db/orders")
orders.first()
type(orders.first())

s = orders.first()
print(s)
type(s)

s[0]
s[:10]
len(s)
s[2:12]

s.split(",")
type(s.split(","))
s.split(",")[0]
s.split(",")[1]
s.split(",")[2]
s.split(",")[3]

# Better to define the data type now
int(s.split(",")[0])
float() etc.. 

int(s.split(",")[1].split(" ")[0].replace("-", ""))
-------------------------------------------------------------------------------------------------------------
Row level transformation- Map
- String Manipulation (Python)
- Row level transformation

orders = sc.textFile("/public/retail_db/orders)
orders.fist()
help(orders.map)

orders.map(lambda o: str(o.split(",")[3])).first()
orders.map(lambda o: int(o.split(",")[1].split(" ")[0].replace("-",""))).first()

#Count by status
orders.map(lambda o: (o.split(",")[3], 1)).first()

orderItems = sc.textFile("/public/retail_db/order_items")
orderItems.first()

for i in orderItem.take(10): print(i)

orderItemsMap = orderItems.map(lambda o: (int(o.split(",")[1]), float(o.split(",")[4]))).first()

for i in orderItemMap.take(10): print(i)

-------------------------------------------------------------------------------------------------------------
Row level transformation- flatMap
- String Manipulation (Python)
- Row level transformation

#FlatMap
LinesList = ["How are you", "let us perform", "word count using flatMap", "to understand flatMap in detail"]
lines = sc.parallelize(LinesList)
type(lines)

for i in lines.take(10): print(i)
word = lines.flatMap(lambda l: l.split())
for i in words.take(20): print(i)
--------------------------------------------------------------------------------------------------------------
# Filtering data using filter
- String Manipulation (python)
- Row level transformations
- Filtering (horizontal and vertical)
-------------------------------------
orders = textFile("/public/retail_db/orders")
orders.first()

for i in order.take(100): print(i)

ordersComplete = orders.filter(lambda o: o.split(",")[3] == "COMPLETE")

for i in ordersComplete.take(100): print(i)
ordersCompleted.count()
orders.count()

ordersComplete = orders.filter(lambda o: o.split(",")[3] == "COMPLETE" or o.split(",")[3] == "CLOSED")
ordersComplete = orders.filter(lambda o: (o.split(",")[3] == "COMPLETE" or o.split(",")[3] == "CLOSED") and o.split(",")[1][:7] == "2014-01")
ordersComplete.count()

orderComplete = orders.filter(lambda o: (o.split(",")[3] in ["COMPLETE", "CLOSED"]) and o.split(",")[1][:7] == "2014-01")
orderComplete.count()
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Joining Datasets- Introduction
- String Manipulation (Python)
- Row level transformations
- Filtering (horizontals and vertical)
- Join
--------------------------------------------
# Inner Join
orders = sc.textFile("/public/retail_db/orders")
orderItems = sc.textFile("/public/retail_db/order_items")

orderMap = orders. \
map(lambda o: (int(o.split(",")[0]), o.split(",")[1]))

orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

ordersJoin = orderMap.join(orderItemsMap)

for i in ordersJoin.take(10): print(i)

---------------------------------------------------
# Outer Join
orders = sc.textFile("/public/retail_db/orders)
orderItems = sc.textFile("/public/retail_db/order_items")

orderMap = orders. \ 
map(lambda o: (int(o.split(",")[0]), o.split(",")[1]))

orderItemMap = orderItems. \ 
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

ordersJoin = orderMap.Join(orderItemsMap)

##############################################
#Rule of thumb for OuterJoins
- ParentTable.leftOuterJoin(ChildTable)
- ChildTable.RightOuterJoin(ParentTable)
##############################################
---------------------------------------------------------------------------------------
# Left Outer Join
orderMap = orders. \
map(lambda o: (int(o.split(",")[0]), o.split(",")[3]))

orderItemMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

orderLeftOuterJoin = orderMap.leftOuterJoin(orderItemsMap)

orderLeftOuterJoinFilter = orderLeftOuterJoin. \
filter(lambda o: o[1][1] == None)

-------------------------------------------------------------------------------------
# Right Outer Join
orderMap = orders. \
map(lambda o: (int(o.split(",")[0]), o.split(",")[3]))

orderItemMap = orderItems. \
map(lambda oi: int(oi.split(",")[1], float(oi.split(",")[4])))

orderRightOuterJoin = orderItemsMap.rightOuterJoin(orderMap)
orderRightOuterJoinFilter = orderRightOuterJoin. \
filter(lambda o: o[1][1] == None)
-------------------------------------------------------------------------------------------------------------------------------------------------------------
# Introduction Aggregations
- String Manipulation (Python)
- Row Level transformation
- Filtering (horizontal and vertical)
- Join
- Aggregations 
----------------------
# Aggregations - total
orderItems = sc.textFile("/public/retail_db/order_items")
orderItems.count()
for i in orderItem.take(10): print(i)

# Aggregations - total - Get revenue for given order_id

orderItemsFiltered = orderItems. \
filter(lambda oi: int(oi.split(",")[1]) == 2)

orderItemsSubtotal = orderItemsFiltered. \
map(lambda oi: float(oi.split(",")[4]))

from operator import add

orderItemsSubtotal.reduce(add)
orderItemsSubtotal.reduce(lambda x, y: x + y)
-----------------------------------

# Get order item details which have minimum order_items_subtotal for given order_id
orderItems = sc.textFile("/public/retail_db/order_items")

for i in orderItems.take(10): print(i)

orderItemsFiltered = orderItems. \
filter(lambda oi: int(oi.split(",")[1]) == 2)

for i in orderItemsFiltered.take(10): print(i)

orderItemsFiltered. \
reduce(lambda x, y: x if (float(x.split(",")[4]) < float(x.split(",")[4])) else y)
-----------------------------------------------------------------------------------------------------
# Get Order Count by status- countByKey 
# hint: This converts RDD type to python class, so this must be the final stage in the transformation

orders = sc.textFiles("/public/retail_db/orders")
for i in orders.take(10): print(i)

orderStatus = orders. \
map(lambda o: (o.split(",")[3], 1))

countByStatus = orderStatus.countByKey()
---------------------------------------------------------------------------------------------
# Aggregations- Understanding combiner- groupByKey
1, (1 to 1000) - sum(1 to 1000) => 1+ 2+ 3+ ....1000
1, (1 to 1000) - sum(1 to 250), sum(251, 500), sum(501, 750), sum(751, 1000)

# Get revenue for each order_id - groupByKey
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

for i in orderItemsMap.take(10): print(i)

orderItemsGroupByOderId = orderItemsMap.groupByKey()
revenuePerOrderId = orderItemsGroupByOrderId. \
map(lambda oi: (oi[0], round(sum(oi[1]))))

for i in revenuePerOrder.take(10): print(i)
------------------------------------------------------------------------------------------------
# Get order item details in descending order by revenue - groupByKey
orderItems = sc.textFile("/public/retail_db/order_items")

for i in orderItems.take(10): print(i)

orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), oi))
for i in orderItemsMap.take(10): print(i)

orderItemsGroupByOrderId = orderItemsMap.groupByKey()
for i in orderItemsGroupByOrderId.take(10): print(i)


orderItemsSortedBySubtotalPerOrder = orderItemsGroupByOrderId. \
flatMap(lambda oi: sorted(oi[1], key=lambda k: float(k.split(",")[4]), reverse=True))

for i in orderItemsSortedBySubtotalPerOrder.take(10): print(i)

orderItemsSortedBySubtotalPerOrder = orderItemsGroupByOrderId. \
Map(lambda oi: sorted(oi[1], key=lambda k: float(k.split(",")[4]), reverse=False))

for i in orderItemsSortedBySubtotalPerOrder.take(10): print(i)

#########################################
### Map is one to one Operation       ###
### FlatMap is one to many operation  ###
#########################################
---------------------------------------------------------------------------------------------
# Aggregations- reduceByKey- Get Revenue for each order id

orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

revenuePerOrderId = orderItemsMap. \
reduceByKey(lambda x, y: x + y)

from operator import add
revenuePerOrderId = orderItemsMap. \
reduceByKey(add)

-----------------------------------------------------------------------------------
# Get order item details with minimum subtotal for each order_id
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), oi))

minSubtotalPerOrderId = orderItemsMap. \
reduceByKey(lambda x, y: x if(float(x.split(",")[4]) < float(y.split(",")[4])) else y)

for i in minSubtotalPerOrderId.take(10): print(i)

-------------------------------------------------------------------------------------
# Get revenue and count of items for each order id
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

for i in orderItemsMap.take(10): print(i)

revenuePerOrder = orderItemsMap. \
aggregateByKey((0.0, 0),
lambda x, y: (x[0] + y, x[1] + 1),
lambda x, y: (x[0] + y[0], x[1] + y[1]))
---------------------------------------------------------------------------------------
# Sorting -sortByKey- Sort data by product price

products = sc.textFile("/public/retail_db/products")
for i in products.take(10): print(i)
productsMap = products. \
filter(lambda p: p.split(",")[4] != ""). \
map(lambda p: (float(p.split(",")[4]), p))

productsSortedByPrice = productsMap.sortByKey()

productsSortedMap = productsSortedByPrice. \
map(lambda p: p[1])
-----------------------------------------------------------------
# Sorting -sortByKey- Sort data by category id and then by price descending
# Sort data by product category and then product price descending - sortByKey

products = sc.textFile("/public/retail_db/products")

ProductsMap = products. \
filter(lambda p: p.split(",")[4] != ""). \
map(lambda p: ((int(p.split(",")[1]), float(p.split(",")[4])), p ))

for i in productsMap. \
sortByKey(). \
map(lambda p: p[1]). \
take(1000): print(i)

---------------------------------------------------------------------------------------------------------
# Ranking- Global Ranking using sortByKey and take
# Get top N products by price- Global Ranking - sortByKey and take

products = sc.textFile("/public/retail_db/products")
productsMap = products. \
filter(lambda p: p.split(",")[4] != ""). \
map(lambda p: (float(p.split(",")[4]), p))
productsSortedByPrice = productsMap.sortByKey(False)

for i in productsSortedByPrice. \
map(lambda p: p[1]). \
take(5): print(i)
----------------------------------------------------------------------------
# Ranking- Global Ranking using sorByKey and take
# Get top N products by price - Global Ranking - top or takeOrdered

products = sc.textFile("/public/retail_db/products")
productsFiltered = products. \
filter(lambda p: p.split(",")[4] != "")

topNProducts = productsFiltered.top(5, key=lambda k: -float(k.split(",")[4]))
for i in topNProducts: print(i)

topNProducts = productsFiltered.takeOrdered(5, key=lambda k: -float(k.split(",")[4]))
for i in topNProducts: print(i)
------------------------------------------------------------------------------
# Ranking -By Key -Get top N products by price per category
# Get top N products by price with in each category - By key Ranking - groupByKey and flatMap

products = sc.textFile("/public/retail_db/products")
productsFiltered = products. \
filter(lambda p: p.split(",")[4] != "")
for i in productsFiltered.take(100): print(i)

productsMap = productsFiltered. \
map(lambda p: (int(p.split(",")[1]), p))
productsGroupsByCategoryId = productsMap.groupByKey()

for i in productsGroupsByCategoryId.take(10): print(i)

######### Detour ###########
# t = productGroupByCategoryId.take(10): print(i)
# l = sorted(t[1], key=lambda k: float(k.split(",")[]4), reverse=True)
# l[:3]
############################

topNProductsByCategory = productsGroupsByCategoryId. \
flatMap(lambda p:
    sorted(p[1], key=lambda k: float(k.split(",")[4]), reverse=True)[:3]
)
for i in topNProductsByCategory.take(10): print(i)

--------------------------------------------------------------------------------------------------------------
# Get top N priced products - By Key Ranking
# Using groupByKey and flatMap

products = sc.textFile("/public/retail_db/products")
productsFiltered = products. \
filter(lambda p: p.split(",")[4] != "")

productsMap = productsFiltered. \
map(lambda p: (int(p.split(",")[1]), p))

productsGroupByCategoryId = productsMap.groupByKey()
for i in productsGroupByCategoryId.take(10): print(i)

t = productsGroupByCategoryId. \
filter(lambda p: p[0] == 59). \
first()

--------------------------------------------------------------
# Ranking- By Key- Get top N priced products- Create Function

def getTopPricedProductsPerCategoryId(productsPerCategoryId, topN):
    productsSorted = sorted(productsPerCategoryId[1],
                        key = lambda k: float(k.split(",")[4]),
                        reverse=True)
    productPrices = map(lambda p: float(p.split(",")[4]), productsSorted)
    topNPrices = sorted(set(productPrices), reverse=True)[:topN]
    import itertools as it
    return it.takewhile(lambda p: float(p.split(",")[4]) in topNPrices, productsSorted)

topNPricedProducts = productsGroupByCategoryId. \
flatMap(lambda p: getTopPricedProductsPerCategoryId(p,3))

---------------------------------------------------------------------------------------
# Set operations- Prepare Data- subsets of products for 2013-12 and 2014-01
orders = sc.textFile("/public/retail_db/orders")
orderItems = sc.textFile("/public/retail_db/order_items")

orders201312 = orders. \
filter(lambda o: o.split(",")[1][:7] == "2013-12"). \
map(lambda o: (int(o.split(",")[0]), o))

orders201401 = orders. \
filter(lambda o: o.split(",")[1][:7] == "2014-01"). \
map(lambda o: (int(o.split(",")[0]), o))

orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), oi))


order201312Join = orders201312.join(orderItemsMap)
order201401Join = orders201401.join(orderItemsMap)

orderItems201312 = orders201312. \
join(orderItemsMap). \
map(lambda oi: oi[1][1])

orderItems201401 = orders201401. \
join(orderItemsMap). \
map(lambda oi: oi[1][1])
----------------------------------------------------------------------------------------------------------------
# Set operations - Union - Get products ids sold in 2013-12 and 2014-01
 products201312 = orderItems201312. \
 map(lambda p: int(p.split(",")[2]))
 
 products201401 = orderItems201401. \
 map(lambda p: int(p.split(",")[2]))

 allProducts = products201312.union(products201401).distinct()

-----------------------------------------------------------------------------------------------------------------
# Set operations - Intersection - Get products ids sold in both 2013-12 and 2014-01
products201312 = orderItems201312. \
map(lambda p: int(p.split(",")[2]))

products201401 = orderItems201401. \
map(lambda p: int(p.split(",")[2]))

commonProducts = products201312.intersection(products201401)
-----------------------------------------------------------------------------------------------------------------
# Set operations - minus - Get products ids sold in 2013-12 but not in 2014-01

products201312only = products201312. \
subtract(products201401). \
distinct()

products201401only = products201401. \
subtract(products201312). \
distinct()

productsSoldOnlyInOneMonth = products201312only. \
union(products201401only)
--------------------------------------------------------------------------------------------------------------------
# Saving RDD back to RDD
# Saving data into HDFS- text file format
- Make sure data is saved with proper delimiters 
- Compression
---------------------------------------
# Saving as text file with delimiters - revenue per order id
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

from operator import add
revenuePerOrderId = orderItemsMap. \
reduceByKey(add). \
map(lambda r: str(r[0]) + "\t" + str(r[1]))

revenuePerOrderId.saveAsTextFile("/user/harishmohan/revenue_per_order_id")

------------------------
exit() # exit pyspark

hadoop fs -ls /user/harishmohan/revenue_per_order_id
hadoop fs -tail /user/harishmohan/revenue_per_order_id/part-00000

for i in sc.textFile("user/harishmohan/revenue_per_order_id").take(10): print(i)
--------------------------------------------------------------------------------------------------------------------------
# Compressions for a given cluster must be configured to support
# Check the hadoop conf file
cd /etc/hadoop/conf
v1 core-site.xml # search: /codecs (org.apache.hadoop.io.compress.SnappyCodec)

revenuePerOrderId.saveAsTextFile("/user/harishmohan/revenue_per_order_compressed", compressionCodecClass= "org.apache.hadoop.io.compress.SnappyCodec")

# File testing
sc.textFile("/user/harishmohan/revenue_per_order_compressed").take(10)
---------------------------------------------------------------------------------------------------------------------------
# Save data in different file formats
- Supported file format
    - orc
    - json
    - parquet
    - avro (with databricks plugin) #databricks plugins are open source
- Steps to save into different file formats
    - Make sure data iis represented as Data Frame 
    - Use write or save API to save Data Frame into different file formats
    - Use compression algorithm if required
-------------------------------------
# Saving as JSON - Get revenue per order id -- Save and write
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))

from operator import add
revenuePerOrderId = orderItemsMap. \
reduceByKey(add). \
map(lambda r: (r[0], round(r[1], 2)))

revenuePerOrderIdDF = revenuePerOrderId.toDF(schema=["order_id", "order_revenue"])

revenuePerOrderIdDF.save("/user/harishmohan/revenue_per_order_json", "json")
revenuePerOrderIdDF.write.json"("/user/harishmohan/revenue_per_order_json.json")

sqlContext.read.json("/user/harishmohan/revenue_per_order_json")









